{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mltools\n",
    "\n",
    "mlflow.set_tracking_uri('http://localhost:5001')\n",
    "\n",
    "# production data\n",
    "target_df = pd.read_parquet('/Users/ondrejgutten/Work/PISI.nosync/data/SLSP/SLSP24abc_to_classify.parquet')\n",
    "target_noplaceholders_df = pd.read_parquet('/Users/ondrejgutten/Work/PISI.nosync/data/SLSP/SLSP24abc_to_classify_noplaceholders_preprocessing.parquet')\n",
    "\n",
    "orig_texts = np.array(target_df.Text)\n",
    "texts = np.array(target_noplaceholders_df.Text)\n",
    "\n",
    "pyfunc_model = mlflow.pyfunc.load_model('models:/SLSP_Spisy_naked_model/1')\n",
    "model = pyfunc_model._PyFuncModel__model_impl.python_model.model\n",
    "vects = model.clf.estimator['vect'].transform(orig_texts)\n",
    "\n",
    "\n",
    "cluster_analysis_df = pd.read_csv('data/SLSP/DBSCAN_clustering_eps0.12_minsamples3_noplaceholders_texts_SLSP24abc.csv')\n",
    "preds = cluster_analysis_df['predictions']\n",
    "clusters = cluster_analysis_df['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster target data\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.12, min_samples=3, metric = 'cosine')\n",
    "vects = model.clf.estimator['vect'].transform(target_df.Text)\n",
    "clusters = dbscan.fit_predict(vects)\n",
    "\n",
    "orig_texts = target_df.Text\n",
    "texts = noplaceholders_preprocessor.predict(orig_texts)\n",
    "preds = model.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected global variables:\n",
    "#   preds\n",
    "#   clusters\n",
    "#   model\n",
    "#   texts\n",
    "#   orig_texts\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def entropy1(labels, base=None):\n",
    "   value,counts = np.unique(labels, return_counts=True)\n",
    "   return entropy(counts, base=base)\n",
    "\n",
    "def cluster_analysis(cluster_number : int):\n",
    "    global preds\n",
    "    global clusters\n",
    "    global model\n",
    "    global texts\n",
    "    probas = model.predict_proba(texts[clusters == cluster_number])\n",
    "    max_probas = np.max(probas, axis = 1)\n",
    "\n",
    "    print(Counter(preds[clusters == cluster_number]))\n",
    "    print(entropy1(preds[clusters == cluster_number]))\n",
    "    print(\"Max proba quantiles (1 - 0.9 - 0.1 - 0) :\")\n",
    "    print(np.quantile(max_probas, 1), np.quantile(max_probas, 0.9), np.quantile(max_probas, 0.1), np.quantile(max_probas, 0))\n",
    "    return(sum(clusters == cluster_number), Counter(preds[clusters == cluster_number]), entropy1(preds[clusters == cluster_number]), np.quantile(max_probas, 1), np.quantile(max_probas, 0))\n",
    "\n",
    "def cluster_inspection(cluster_number : int, show_texts : bool = True):\n",
    "    global texts\n",
    "    global orig_texts\n",
    "    global clusters\n",
    "    global preds\n",
    "    global model\n",
    "    probas = model.predict_proba(texts[clusters == cluster_number])\n",
    "    top2_indices = np.argsort(-probas)[:,:2]\n",
    "    top2_probas = probas[np.arange(probas.shape[0])[:, None], top2_indices]\n",
    "    top2_labels = model.known_labels[top2_indices]\n",
    "    cluster_texts = texts[clusters == cluster_number]\n",
    "    cluster_orig_texts = orig_texts[clusters == cluster_number]\n",
    "\n",
    "    for idx in range(len(cluster_texts)):\n",
    "        if show_texts:\n",
    "            print(cluster_orig_texts[idx])\n",
    "        print(top2_probas[idx])\n",
    "        print(top2_labels[idx])\n",
    "    \n",
    "    if not show_texts:\n",
    "        print(cluster_orig_texts[0])\n",
    "\n",
    "def maximum_within_cluster_distance(cluster_number : int):\n",
    "    global vects\n",
    "    subset = vects[clusters == cluster_number]\n",
    "    distances =  np.zeros((subset.shape[0],subset.shape[0]))\n",
    "    for idx in range(subset.shape[0]):\n",
    "        for idx2 in range(idx):\n",
    "            if idx != idx2:\n",
    "                distances[idx, idx2] = 1 - cosine_similarity(subset[idx], subset[idx2])\n",
    "    return np.max(distances)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis of heterogeneous clusters\n",
    "# cluster 22\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "i = 22\n",
    "\n",
    "dbscan_models = {}\n",
    "subset_clusters = {}\n",
    "\n",
    "dbscan_models[i] = DBSCAN(eps = 0.2, min_samples=3)\n",
    "subset_texts = orig_texts[clusters == i]\n",
    "subset_vects = vects[clusters == i]\n",
    "subset_clusters[i] = dbscan_models[i].fit_predict(subset_vects)\n",
    "\n",
    "orig_texts[clusters == i][subset_clusters[i] == 0]\n",
    "# cluster16\n",
    "# cluster133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_number_to_idxs(cluster_number : int):\n",
    "    return np.where(clusters == cluster_number)[0]\n",
    "\n",
    "def cluster_subset_to_idxs(cluster_number : int, subset_index : list[bool]):\n",
    "    subset_index = np.array(subset_index)\n",
    "    return np.where((clusters.to_numpy() == cluster_number))[0][subset_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_map(func, data, num_workers=8, **kwargs):\n",
    "    import multiprocessing\n",
    "    import functools\n",
    "\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"fork\", force = True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    func_with_kwargs = functools.partial(func, **kwargs)\n",
    "\n",
    "    \"\"\"Runs a function in parallel using multiprocessing.Pool with fork.\"\"\"\n",
    "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
    "        results = pool.map(func_with_kwargs, data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the closest sample from one of the clustered samples\n",
    "#def find_closest_clustered_sample(vects: np.ndarray, idx: int, clusters: list[int]):\n",
    "similarities = cosine_similarity(vects[idx], vects[clusters != -1])[0]\n",
    "max_idx = similarities.argmax()\n",
    "print(similarities[max_idx])\n",
    "print(clusters[clusters != -1][max_idx])\n",
    "print(orig_texts[idx])\n",
    "print(orig_texts[clusters != -1][max_idx])\n",
    "\n",
    "\n",
    "def sims(vects, clusters, idx):\n",
    "    similarities = cosine_similarity(vects[idx], vects[clusters != -1])[0]\n",
    "    max_idx = similarities.argmax()\n",
    "    return similarities[max_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect similarity of unclustered samples to clustered samples using cosine_similarity + fuzz similarity\n",
    "\n",
    "def max_cosine_similarity(idx, cluster_number):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    global vects\n",
    "    global clusters\n",
    "    global orig_texts\n",
    "    sims = cosine_similarity(vects[idx],vects[clusters == cluster_number])\n",
    "    max_value = np.max(sims)\n",
    "    max_idx = np.where(clusters == cluster_number)[0][np.argmax(sims)]\n",
    "    return max_value, max_idx\n",
    "\n",
    "def max_fuzz_similarity(idx, cluster_number, texts_to_use = None):\n",
    "    from rapidfuzz import fuzz\n",
    "    global vects\n",
    "    global clusters\n",
    "    global orig_texts\n",
    "    texts_to_use = orig_texts if texts_to_use is None else texts_to_use\n",
    "    sims = [fuzz.token_sort_ratio(texts_to_use[idx],text) for text in texts_to_use[clusters == cluster_number]]\n",
    "    max_value = np.max(sims)\n",
    "    max_idx = np.where(clusters == cluster_number)[0][np.argmax(sims)]\n",
    "    return max_value, max_idx\n",
    "\n",
    "def best_cluster_match(idx, metric = 'cosine', texts_to_use = None):\n",
    "    if clusters[idx] != -1:\n",
    "        return clusters[idx]\n",
    "    max_cluster_number = max(np.unique(clusters))\n",
    "\n",
    "    def max_fuzz_length_similarity(idx, cluster_number):\n",
    "        return max_fuzz_similarity(idx, cluster_number, texts_to_use)\n",
    "\n",
    "    metric = max_cosine_similarity if metric == 'cosine' else max_fuzz_length_similarity\n",
    "    sims = np.array([metric(idx, i)[0] for i in range(max_cluster_number + 1)])\n",
    "    return np.max(sims), np.argmax(sims), sims\n",
    "\n",
    "def new_max_cosine_similarity(text, cluster_number):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    global vects\n",
    "    global clusters\n",
    "    global orig_texts\n",
    "    sims = cosine_similarity(model.clf.estimator['vect'].transform([text]),vects[clusters == cluster_number])\n",
    "    max_value = np.max(sims)\n",
    "    max_idx = np.where(clusters == cluster_number)[0][np.argmax(sims)]\n",
    "    return max_value, max_idx\n",
    "\n",
    "def new_max_fuzz_similarity(text, cluster_number, texts_to_use = None):\n",
    "    from rapidfuzz import fuzz\n",
    "    global vects\n",
    "    global clusters\n",
    "    global orig_texts\n",
    "    texts_to_use = orig_texts if texts_to_use is None else texts_to_use\n",
    "    sims = [fuzz.token_sort_ratio(text,cluster_text) for cluster_text in texts_to_use[clusters == cluster_number]]\n",
    "    max_value = np.max(sims)\n",
    "    max_idx = np.where(clusters == cluster_number)[0][np.argmax(sims)]\n",
    "    return max_value, max_idx\n",
    "\n",
    "def new_best_cluster_match(text, metric = 'cosine', texts_to_use = None):\n",
    "    max_cluster_number = max(np.unique(clusters))\n",
    "\n",
    "    def new_max_fuzz_length_similarity(text, cluster_number):\n",
    "        return new_max_fuzz_similarity(text, cluster_number, texts_to_use)\n",
    "\n",
    "    metric = new_max_cosine_similarity if metric == 'cosine' else new_max_fuzz_length_similarity\n",
    "    sims = np.array([metric(text, i)[0] for i in range(max_cluster_number + 1)])\n",
    "    return np.max(sims), np.argmax(sims), sims\n",
    "\n",
    "def new_parallel_best_cluster_match(text, metric = 'cosine', texts_to_use = None):\n",
    "    max_cluster_number = max(np.unique(clusters))\n",
    "\n",
    "    def new_max_fuzz_length_similarity(text, cluster_number):\n",
    "        return new_max_fuzz_similarity(text, cluster_number, texts_to_use)\n",
    "\n",
    "    metric = new_max_cosine_similarity if metric == 'cosine' else new_max_fuzz_length_similarity\n",
    "    sims = np.array([metric(text, i)[0] for i in range(max_cluster_number + 1)])\n",
    "    return np.max(sims), np.argmax(sims), sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TLDR - vsetky predikovane na orig_texts ako 7499 alebo predikovane na prep_texts ako 7499 su spravne oznacene ako 7499,s tymito vynimkami:\n",
    "\n",
    "# VYNIMKY z 7499. Vsetky ostatne z 7499 v clusteri -1 su spravne oznacene ako 7499\n",
    "# z predikcii na orig_texts su tieto 2 vynimky:\n",
    "#idx 37324\n",
    "'POVOLENIE NÁ POBYŤ O RNS255036 Kód zdravotnej poisťovne: 25 \" MEHMET GURÚ PISKÍN ROd\"šČŠIŠ ž&äoängéžlĺ\\\\i\\\\ňkačné číslo poistenca: Dam\"ä\\'šuj%ežŕ,t 983 :, . Preukazplatnýcc — 01,10,2014 J DÁTUM A MIESTO NARODENIA, D3.12.1983 \"TUR, ERUH: POHLAVIE ŠTÁTNA PRÍSLUŠNOSŤ M TUREGCKO POZNÁMKY IRSVKRN525503628312039241<<<<< 8312033M1909225TUR<<<<<<<<<<<5 PISKIN<<MEHMET<GU_ERUE<<<<<<<<< k š > ZÁZNAMY POISŤOVNE: Preukaz číslo: 25-030100963 Plati do: Kôd poisťovne - pobočky: 25 - 03 Podpis a odtlačok pečiatky',\n",
    "\n",
    "#idx 36599\n",
    "\"Dodatok k PRACOVNEJ ZMLUVE Dohoda o zmene pracovnej zmluvy Zamestnávateľ : [Firma] Potravinárska č.6,[PSC]Rimavská Sobota Zastúpený: Ing. [Meno Priezvisko], predseda predstavenstva a generálny riaditeľ, Ing. [Meno Priezvisko], PhD., finančný riaditeľ a Pán (ďalej zamestnanec: [Meno Priezvisko] narodený [DatumNarodenia] [RodneCislo] číslo OP: EU463988 trvalé bydlisko[UlicaCislo] [Mesto]uzatvorili túto dohodu o zmene pracovnej zmluvy: 1. Písomná pracovná zmluvá uzavretá medzi zamestnancom a zamestnávateľom dňa: 01.01.2021 sa mení dňom 091.07.2021 takto: 2. Pracovný pomer na dobu určitú do 30.06.2021 sa mení na dobu určitú do 31.12.2021. Ostatné podmienky dohodnuté v pracovnej zmluve ostávajú nezmenné. U Rimavská Sobota, dňa 10.06.2021 TAURIS, as ' Potravinárska 6 á :[PSC]Rimäská Sobola — d / [/ —— pečiatka a podpis |(zamieštnávateľa Ť - podpis zamestnanca . Strana 1z1\",\n",
    "\n",
    "# z predikcii na prep_texts ktore nie su predikovane ako 7499 na orig_texts je tato 1 vynimka:\n",
    "# 37362\n",
    "'ÚRAD PRÁCE, [Meno Priezvisko] A RODINY [Meno Priezvisko] Farbiarska 57, [Mesto] Potvrdenie o poberaní rodičovského príspevku za rok 2022 Meno a priezvisko: [Meno Priezvisko] Dátum narodenia: [DatumNarodenia] Adresa:[UlicaCislo] [PSC] [Meno Priezvisko] Dávky za jednotlivé mesiace Mesiac Platba v eurách Január 383,80 Február — 383,80 Marec — - j W Ap P 383,80 Máj 383,80 Jún 383,80 Júl 383,80 [Meno Priezvisko])(í | September - o W Október 0,00 | November ob December | 0,00 Suma za celé obdobie: 2 686,60 SOCIÁLNYCH MEÍ A áž SOSVČTARÁ UUBOVŇA Farbiarska ŠT [PSC] [Meno Priezvisko] V Starej Ľubovni 11.07.2022 | O / ot S Podpis zodpovedn%o pracovníka Vybavuje: [Meno Priezvisko], Mgr. PhD.',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect similarity of unclustered samples to clustered samples using cosine_similarity + fuzz similarity\n",
    "idxs1 = np.where(clusters == -1)[0]\n",
    "mydf = pd.DataFrame(columns = ['max_cosine_similarity','similar_cosine_cluster','model_cosine_prediction','max_fuzz_similarity','similar_fuzz_cluster','model_fuzz_prediction'], index = idxs1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "target_df = pd.read_parquet('/Users/ondrejgutten/Work/PISI.nosync/data/SLSP/SLSP24abc_to_classify.parquet')\n",
    "orig_texts = target_df.Text.to_numpy()\n",
    "chopped_texts = np.array([text[:1000] for text in orig_texts])\n",
    "def fuzz_similarity(sample_1, sample_2):\n",
    "    return fuzz.token_sort_ratio(sample_1, sample_2)\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def generate_indices(n):\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            yield (i, j)\n",
    "\n",
    "def compute_distance(x, y):\n",
    "    \"\"\"Example distance function (Euclidean distance).\"\"\"\n",
    "    return fuzz_similarity(x, y)\n",
    "\n",
    "def parallel_distance_matrix(data, num_workers=4, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Computes the distance matrix in parallel using a generator to avoid memory issues.\n",
    "    \n",
    "    - `num_workers`: Number of parallel jobs.\n",
    "    - `batch_size`: Number of (i, j) pairs processed in one batch.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    dist_matrix = np.zeros((n, n))  # Initialize result matrix\n",
    "\n",
    "    print(\"Number of pairs: \", n * (n - 1) // 2)\n",
    "    batch = []\n",
    "    for i, j in generate_indices(n):\n",
    "        batch.append((i, j))\n",
    "\n",
    "        # Process when we reach batch size\n",
    "        if len(batch) >= batch_size:\n",
    "            print('Processing batch starting with', batch[0])\n",
    "            results = Parallel(n_jobs=num_workers, backend=\"loky\")(\n",
    "                delayed(compute_distance)(data[i], data[j]) for i, j in batch\n",
    "            )\n",
    "\n",
    "            # Store results and clear batch\n",
    "            for (i, j), dist in zip(batch, results):\n",
    "                dist_matrix[i, j] = dist\n",
    "                dist_matrix[j, i] = dist  # Symmetric fill\n",
    "            \n",
    "            batch = []  # Reset batch\n",
    "\n",
    "    # Process any remaining pairs\n",
    "    if batch:\n",
    "        results = Parallel(n_jobs=num_workers, backend=\"loky\")(\n",
    "            delayed(compute_distance)(data[i], data[j]) for i, j in batch\n",
    "        )\n",
    "        for (i, j), dist in zip(batch, results):\n",
    "            dist_matrix[i, j] = dist\n",
    "            dist_matrix[j, i] = dist\n",
    "\n",
    "    return dist_matrix\n",
    "\n",
    "# Example usage\n",
    "data = chopped_texts  # 100 samples, 5-dimensional vectors\n",
    "dist_matrix = parallel_distance_matrix(data, num_workers=12, batch_size=100000)\n",
    "print(dist_matrix)\n",
    "print(\"Distance matrix computation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_clusters_to_labels - read from excel sheet - group cluster analysis\n",
    "group_clusters_to_labels = pd.read_clipboard(sep = ';')\n",
    "\n",
    "# group_clusters_to_idxs\n",
    "group_clusters_to_idxs = joblib.load('/Users/ondrejgutten/Work/PISI.nosync/data/SLSP/group_clusters_to_idxs.pickle')\n",
    "\n",
    "# excel_labels - semi-manual assignment of labels in excel sheet\n",
    "excel_labels = pd.read_clipboard()\n",
    "excel_labels.columns = ['similar_cosine_cluster','prelabel']\n",
    "excel_labels['label'] = excel_labels['prelabel']\n",
    "excel_labels.loc[excel_labels['prelabel'] == 'max_cosine_sim_cluster', 'label'] = ['cluster_' + str(x) for x in excel_labels.loc[excel_labels['prelabel'] == 'max_cosine_sim_cluster', 'similar_cosine_cluster']]\n",
    "\n",
    "def cluster_number_to_cluster_group_idx(cluster_number):\n",
    "    found = np.where([cluster_number in cluster_list for cluster_list in cluster_groups])[0]\n",
    "    if found.shape[0] == 0:\n",
    "        return None\n",
    "    if found.shape[0] > 1:\n",
    "        raise ValueError('Multiple clusters found')\n",
    "    return found[0]\n",
    "\n",
    "def idx_to_cluster_group_idx(idx):\n",
    "    found = np.where([idx in cluster_list for cluster_list in group_clusters_to_idxs])[0]\n",
    "    if found.shape[0] == 0:\n",
    "        return None\n",
    "    if found.shape[0] > 1:\n",
    "        raise ValueError('Multiple clusters found')\n",
    "    return found[0]\n",
    "\n",
    "def idx_to_final_label(idx):\n",
    "    # look for idx in group_clusters_to_idxs\n",
    "    # if successful, return label from corresponding group_clusters_to_labels\n",
    "    # if not look in excel_labels['label']\n",
    "        # if label starts with cluster_X, return label corresponding to group_clusters_to_labels[ cluster_number_to_cluster_group_idx(X) ]\n",
    "        # otherwise return excel_labels['label']\n",
    "    \n",
    "    clustered = idx_to_cluster_group_idx(idx)\n",
    "    if clustered is not None:\n",
    "        return group_clusters_to_labels[clustered]\n",
    "    else:\n",
    "        label = excel_labels.loc[idx,'label']\n",
    "        if 'cluster' in label:\n",
    "            cluster_number = int(label.split('_')[1])\n",
    "            return group_clusters_to_labels[cluster_number_to_cluster_group_idx(cluster_number)]\n",
    "        return label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
