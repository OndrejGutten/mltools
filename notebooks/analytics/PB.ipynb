{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02/2025\n",
    "# Spisy z Postovej Banky\n",
    "# Mame vela 'INE', ktore nie su az tak dolezite.\n",
    "# Mame vela s viacerymi prilohami, ktore je OK poslat na manual.\n",
    "# Mame dorucenky, ktore nevieme dalej triedit a posielame na manual.\n",
    "\n",
    "# Chceme kategoriu INE, MULTIDOCUMENT (vsetko v kat2 s ozcnacenim 'viacero priloh'), DORUCENKA (vsetky dorucenky).\n",
    "# Toto bude hlavny model. Dalsie triedenie je optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mltools\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:4000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mltools\n",
    "from copy import copy\n",
    "\n",
    "# Get raw data\n",
    "raw_df = pd.read_parquet('/Users/ondrejgutten/Work/PISI.nosync/data/PB/SpisyPB18-PB24_v3.parquet')\n",
    "\n",
    "# preprocess texts\n",
    "initial_preprocessor = mlflow.pyfunc.load_model('models:/InitialTextPreprocessor/1')\n",
    "noplaceholders_preprocessor = mlflow.pyfunc.load_model('models:/InitialAndRemovePlaceholdersTextPreprocessor/1')\n",
    "\n",
    "# Extract useful data (text + categories)\n",
    "placeholders_df = copy(raw_df.iloc[:,[2,5]])\n",
    "placeholders_df['Text'] = initial_preprocessor.predict(placeholders_df['Text'])\n",
    "\n",
    "noplaceholders_df = copy(raw_df.iloc[:,[2,5]])\n",
    "noplaceholders_df['Text'] = noplaceholders_preprocessor.predict(noplaceholders_df['Text'])\n",
    "\n",
    "mydf = noplaceholders_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = mltools.data.drop_classes_with_1_member(mydf,target_column = mydf.columns[1])\n",
    "train_df, test_df = mltools.data.pandas_split_categorical_data(mydf)\n",
    "\n",
    "X = mydf.loc[:,mydf.columns[0]]\n",
    "y = mydf.loc[:,mydf.columns[1]]\n",
    "train_X = train_df.loc[:,train_df.columns[0]]\n",
    "train_y = train_df.loc[:,train_df.columns[1]]\n",
    "test_X = test_df.loc[:,test_df.columns[0]]\n",
    "test_y = test_df.loc[:,test_df.columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by 'Balik'\n",
    "\n",
    "list_dfs_split_by_package = [df for df in mydf.groupby(raw_df['BalÃ­k'])]\n",
    "\n",
    "# Mame baliky PB18a,PB18b,PB18c,PB21,PB24. V tomto poradi vyskusame pouzit prvych N ako trenovaci set a zvysok ako testovaci set\n",
    "# Specifikujeme volbu stringom 'pa', 'pb','pc','p21','p24'\n",
    "\n",
    "train_set_specification = 'p21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if train_set_specification == 'pa':\n",
    "    train_set = [0]\n",
    "    test_set = [1,2,3,4]\n",
    "elif train_set_specification == 'pb':\n",
    "    train_set = [0,1]\n",
    "    test_set = [2,3,4]\n",
    "elif train_set_specification == 'pc':\n",
    "    train_set = [0,1,2]\n",
    "    test_set = [3,4]\n",
    "elif train_set_specification == 'p21':\n",
    "    train_set = [0,1,2,3]\n",
    "    test_set = [4]\n",
    "elif train_set_specification == 'p24':\n",
    "    train_set = [4]\n",
    "    test_set = [0,1,2,3]\n",
    "else:\n",
    "    print('Invalid train_set_specification')\n",
    "    exit()\n",
    "\n",
    "train_X =pd.concat([list_dfs_split_by_package[i][1].iloc[:,0] for i in train_set]) \n",
    "train_y = pd.concat([list_dfs_split_by_package[i][1].iloc[:,1] for i in train_set])\n",
    "test_X = pd.concat([list_dfs_split_by_package[i][1].iloc[:,0] for i in test_set])\n",
    "test_y = pd.concat([list_dfs_split_by_package[i][1].iloc[:,1] for i in test_set])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS TO TRY:\n",
    "- Single-label - multidoc ako samostatna kategoria\n",
    "- Multilabel bez zlomov stran\n",
    "- OCR quality ako prediktor zleho scanu? - Text uz obsahuje indikaciu zleho scanu\n",
    "- nedelit dokumenty train_test splitom, ale podla balikov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = mltools.architecture.TF_IDF_XGBoost_Classifier('xgb',{})\n",
    "xgb.fit(train_X, train_y)\n",
    "model = xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = mltools.architecture.TF_IDF_Vectorizer_RF('rf')\n",
    "rf.fit(train_X, train_y)\n",
    "model = rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(test_X)\n",
    "accuracy_score(test_y, y_pred)\n",
    "\n",
    "# full train_test_split\n",
    "# xgb = 0.7 \n",
    "# RF = 0.999\n",
    "\n",
    "# train_X = pa\n",
    "# xgb = 0.81\n",
    "# rf = 0.879\n",
    "\n",
    "# train_X = pb\n",
    "# rf = 0.856\n",
    "\n",
    "# train_X = pc\n",
    "# rf = 0.855\n",
    "\n",
    "# train_X = p21\n",
    "# rf = 0.898\n",
    "\n",
    "# train_X = p24\n",
    "# rf = 0.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrf = mltools.architecture.TF_IDF_Multilabel_Classifier(rf,'multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract useful data (text + categories)\n",
    "mydf = raw_df.iloc[:,[2,4]]\n",
    "\n",
    "all_categories = mydf.iloc[:,1].unique()\n",
    "single_doc_categories = [cat for cat in all_categories if '~' not in cat]\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([single_doc_categories])\n",
    "\n",
    "X = mydf.iloc[:,0]\n",
    "y = mlb.transform([x.split('~') for x in mydf.iloc[:,1]])\n",
    "\n",
    "#mydf = mltools.data.drop_classes_with_1_member(mydf,target_column = mydf.columns[1])\n",
    "train_df, test_df = mltools.data.pandas_split_categorical_data(mydf)\n",
    "\n",
    "train_X = train_df.iloc[:,0]\n",
    "train_y = mlb.transform([x.split('~') for x in train_df.iloc[:,1]])\n",
    "test_X = test_df.iloc[:,0]\n",
    "test_y = mlb.transform([x.split('~') for x in test_df.iloc[:,1]])\n",
    "\n",
    "train_multidoc_bool = [sum(train_y[i]) > 1 for i in range(len(train_y))]\n",
    "test_multidoc_bool = [sum(test_y[i]) > 1 for i in range(len(test_y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(train_X, train_y)\n",
    "\n",
    "# multilabel je v test sade len 79 a ma accuracy 0.3.\n",
    "# Presny hit je vzacny, ale v 74/79 pripadov su vsetky predikovane sucastou true labels,\n",
    "# i.e. nizka accuracy je, lebo missneme niektore labels, ale nevymyslame si nove\n",
    "# najcastejsie chyba dorucenka\n",
    "# namatkovo 4679 z raw_df ma mat dorucenku, upozornenie pred vyzvou a hlavnu zmluvu - ale vyzera to len ako hlavna zmluva"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
